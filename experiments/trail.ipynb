{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017c023e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f10138",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_api = os.getenv(\"HUGGING_FACE_KEY\")\n",
    "openai_api=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f972ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_email(api_key,prompt,tone,model='gpt-3.5-turbo'):\n",
    "    if not api_key:\n",
    "        return {\"error\": \"Please enter your OpenAI API key.\", \"response\": None}\n",
    "    try:\n",
    "        llm=OpenAI(api_key=api_key)\n",
    "\n",
    "        # email response\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a professional email assistant. Generate a {tone} tone response.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        response=llm.chat.completions.create(model=model,messages=messages,temperature=.8)\n",
    "        email_response=response.choices[0].message.content\n",
    "\n",
    "        # Subject line\n",
    "        subj_msgs = [\n",
    "            {\"role\": \"system\", \"content\": \"Generate a concise subject line for this email.\"},\n",
    "            {\"role\": \"user\", \"content\": email_response}\n",
    "        ]\n",
    "        subject=llm.chat.completions.create(model=model, messages=subj_msgs, temperature=0.7)\n",
    "        subject_response=subject.choices[0].message.content\n",
    "\n",
    "        # thread summrary\n",
    "        sum_msgs = [\n",
    "            {\"role\": \"system\", \"content\": \"Provide a concise summary of the email thread.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Thread:\\n{prompt}\\n\\nResponse:\\n{email_response}\"}\n",
    "        ]\n",
    "        summary=llm.chat.completions.create(model=model, messages=sum_msgs, temperature=0.7)\n",
    "        thread_summary = summary.choices[0].message.content\n",
    "\n",
    "        return {\"error\": None, \"response\": email_response, \"subject\": subj_msgs, \"summary\": thread_summary}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"response\": None\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d162ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_email(api_key=openai_api,prompt='Genearte a email for friednly match to ronaldo',tone='Friendly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf29b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "/Users/akashmukherjee/Programming/Practise ML/Email assistant/my_env/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_hf_pipelines():\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "    sentiment = pipeline(\"sentiment-analysis\")\n",
    "    ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    return summarizer, generator, sentiment, ner\n",
    "\n",
    "summarizer_hf, generator_hf, sentiment_hf, ner_hf = load_hf_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e0321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_email_with_hf(prompt):\n",
    "    try:\n",
    "        thread_summary = summarizer_hf(prompt, max_length=80, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "        reply_output = generator_hf(f\"Reply to the email: {prompt}\", max_length=256, do_sample=True)[0][\"generated_text\"]\n",
    "        subject_line = generator_hf(f\"Generate a short subject line for: {reply_output}\", max_length=20)[0][\"generated_text\"]\n",
    "        return {\"error\": None, \"response\": reply_output, \"subject\": subject_line, \"summary\": thread_summary}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"response\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73e46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    entities = ner_hf(text)\n",
    "    df = pd.DataFrame([{\"entity\": ent.get(\"entity_group\", ent.get(\"entity\")), \"text\": ent[\"word\"], \"score\": ent.get(\"score\", None)} for ent in entities])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c76a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
